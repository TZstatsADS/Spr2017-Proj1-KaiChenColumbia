---
title: "Kai_Chen_Project_1"
output: html_notebook
---




# Step 0: check and install needed packages. Load the libraries and functions. 

```{r, message=FALSE, warning=FALSE}
packages.used=c("rvest", "tibble", "qdap", 
                "sentimentr", "gplots", "dplyr",
                "tm", "syuzhet", "factoextra", 
                "beeswarm", "scales", "RColorBrewer",
                "RANN", "tm", "topicmodels", "wordcloud", "RColorBrewer", 
                "tidytext", "ggplot2","scales" )

# check packages that need to be installed.
packages.needed=setdiff(packages.used, 
                        intersect(installed.packages()[,1], 
                                  packages.used))
# install additional packages
if(length(packages.needed)>0){
  install.packages(packages.needed, dependencies = TRUE)
}

# load packages
library("rvest")
library("tibble")
library("qdap")
library("sentimentr")
library("gplots")
library("dplyr")
library("tm")
library("syuzhet")
library("factoextra")
library("beeswarm")
library("scales")
library("RColorBrewer")
library("RANN")
library("tm")
library("topicmodels")
library("wordcloud")
library("tidytext")
library("RColorBrewer")
library("ggplot2")
library("scales")


source("../lib/plotstacked.R")
source("../lib/speechFuncs.R")
source("../lib/kai_source.R")
source("../lib/ggradar.R")
```
This notebook was prepared with the following environmental settings.

```{r}
print(R.version)
```

# Step 1: Data harvest: scrap speech URLs from <http://www.presidency.ucsb.edu/>.

In this project, I will use 4 datasets to draw my conclusion. Hence, I will scrap the texts and URLs from 4 pages in Step 1. 


```{r}
## Inauguaral speeches
main.page <- read_html(x = "http://www.presidency.ucsb.edu/inaugurals.php")

# inaug is used to store the text and url of a link 
inaug=f.speechlinks(main.page)
# The last row is abnormal, so we remove it.
inaug <- inaug[-nrow(inaug),] 
inaug.time <- as.Date(inaug[,1], format="%B %e, %Y")


#### Nomination speeches
main.page=read_html("http://www.presidency.ucsb.edu/nomination.php")
nomin <- f.speechlinks(main.page)
nomin.time <- as.Date(nomin[,1], format="%B %e, %Y")


#### Farewell speeches
main.page=read_html("http://www.presidency.ucsb.edu/farewell_addresses.php")
farewell <- f.speechlinks(main.page)
farewell.time <- as.Date(farewell[,1], format="%B %e, %Y")


#### Political Party, which will be explained in Step 5
main.page=read_html("http://www.presidency.ucsb.edu/platforms.php")
twoparty <- c("Democratic","Republican")

## We would pay a little bit more effort to finish the web scrapper 
party <- f.speechlinks(main.page)
party <- party[!(party[,1]=="pdf"),]
party <- party[party[,1]==("Democratic")|(party[,1]=="Republican"),]

# We only use the data after 1856
party <- party[1:(nrow(party)-4),]
year.party <- rep(seq(2016, 1856, -4), each = 2)

# The length of the above vectors should be same!
nrow(party) == length(year.party)
party.list <- data.frame(Year = year.party, 
                         Party = party$links, 
                         url = party$urls, 
                         stringsAsFactors = FALSE)
```


# Step 2: Prepare CSV data sets

We will not create a csv file for the political party data since a function in kai_sourse.R will give us any access to the information we are interested. 

```{r}
inaug.list=read.csv("../data/inauglist.csv", stringsAsFactors = FALSE)
nomin.list=read.csv("../data/nominlist.csv", stringsAsFactors = FALSE)
farewell.list=read.csv("../data/farewelllist.csv", stringsAsFactors = FALSE)
```

# Step 3: scrap the texts of speeches from the speech URLs.


```{r}
speech.list=rbind(inaug.list, nomin.list, farewell.list)
speech.list$type=c(rep("inaug", nrow(inaug.list)),
                   rep("nomin", nrow(nomin.list)),
                   rep("farewell", nrow(farewell.list)))
speech.url=rbind(inaug, nomin, farewell)
speech.list=cbind(speech.list, speech.url)
```    

```{r}
# Loop over each row in speech.list
speech.list$fulltext=NA
for(i in seq(nrow(speech.list))) {
  text <- read_html(speech.list$urls[i]) %>% # load the page
    html_nodes(".displaytext") %>% # isloate the text
    html_text() # get the text
  speech.list$fulltext[i]=text
  # Create the file name
  filename <- paste0("../data/fulltext/", 
                     speech.list$type[i],
                     speech.list$File[i], "-", 
                     speech.list$Term[i], ".txt")
  sink(file = filename) %>% # open file to write 
  cat(text)  # write the file
  sink() # close the file
}
```

```{r}
### Do in the same way for party
party.list$fulltext=NA
for(i in seq(nrow(party.list))) {
  text <- read_html(party.list$url[i]) %>% # load the page
    html_nodes(".displaytext") %>% # isloate the text
    html_text() # get the text
  party.list$fulltext[i]=text
  # Create the file name
  filename <- paste0("../data/PartyText/", 
                     party.list$type[i],
                     party.list$File[i], "-", 
                     party.list$Term[i], ".txt")
  sink(file = filename) %>% # open file to write 
  cat(text)  # write the file
  sink() # close the file
} # This might run slowly.
```


# Step 4: data Processing --- generate list of sentences

When we build our sentences-related variables, we will put sentiment in the structure, which will be used in the following steps. [NRC sentiment lexion](http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm). Like we did in the class, we will assign an sequential id to each sentence in a speech (`sent.id`) and also calculated the number of words in each sentence as *sentence length* (`word.count`).

```{r, message=FALSE, warning=FALSE}
sentence.s.list=NULL
for(i in 1:nrow(speech.list)){
  sentences=sent_detect(speech.list$fulltext[i],
                        endmarks = c("?", ".", "!", "|",";"))
  if(length(sentences)>0){
    emotions=get_nrc_sentiment(sentences)
    word.count=word_count(sentences)
    emotions=diag(1/(word.count+0.01))%*%as.matrix(emotions) #平均每个字
    sentence.s.list=rbind(sentence.s.list, 
                        cbind(speech.list[i,-ncol(speech.list)],
                              sentences=as.character(sentences), 
                              word.count,
                              emotions,
                              sent.id=1:length(sentences)
                              ) 
    )
  }
}
```


```{r, message=FALSE, warning=FALSE}
# party
sentence.p.list=NULL
for(i in 1:nrow(party.list)){
  sentences=sent_detect(party.list$fulltext[i],
                        endmarks = c("?", ".", "!", "|",";"))
  if(length(sentences)>0){
    emotions=get_nrc_sentiment(sentences)
    word.count=word_count(sentences)
    # colnames(emotions)=paste0("emo.", colnames(emotions))
    # in case the word counts are zeros?
    emotions=diag(1/(word.count+0.01))%*%as.matrix(emotions) #平均每个字
    sentence.p.list=rbind(sentence.p.list, 
                        cbind(party.list[i,-ncol(party.list)],
                              sentences=as.character(sentences), 
                              word.count,
                              emotions,
                              sent.id=1:length(sentences)
                              ) 
            
    )
  }
}
```


Some non-sentences exist in raw data due to erroneous extra end-of sentence marks. 
```{r}
sentence.s.list=
  sentence.s.list%>%
  filter(!is.na(word.count))

sentence.p.list=
  sentence.p.list%>%
  filter(!is.na(word.count))

```


# Step 5: Data Analysis --- My Data Story between Nominees and Parties 

From Wikipedia(https://en.wikipedia.org/wiki/Party_platform), a political party platform is a formal set of principal goals which are supported by a political party or individual candidate, in order to appeal to the general public, for the ultimate purpose of garnering the general public's support and votes about complicated topics or issues. 


Through this definition, we can roughly have the knowledge that the aim of a party platform is to make some formal announcements and to bring more votes by showing the public a beautiful blueprint. As a representative of a party, a nominee will usually speak for his party. However, the roles party and its nominees play in the Election will not be totally same, which might lead to the differences in their articles, let it be speeches or texts. In this project, I will mainly use several text-mining tools to explore this phenomenon. 


*a. Word Cloud Analysis*


First, let's try to explore whether the differences really exist, or even can be discovered in an intuitive way. A simple and intuitive method to explore text data is to use word cloud(https://en.wikipedia.org/wiki/Tag_cloud). 


```{r}


## To make R notebook concise, the steps of cleaning data are completed in 
## the function "Create_processed_corpus()", the details of which are in 
## "../lib/kai_source.R"

inaug.path <- "../data/InauguralSpeeches/"
inaug.c <- Create_processed_corpus(inaug.path)
inaug.tdm <- TermDocumentMatrix(inaug.c, 
                                control = list(weighting = function(x)
                                                weightTfIdf(x, 
                                                normalize =F),
                                          stopwords = TRUE))
inaug.tt <- tidy(inaug.tdm)

## Do same to "party"" part

party.path <- "../data/PartyText/"
party.c <- Create_processed_corpus(party.path)
party.tdm <- TermDocumentMatrix(party.c, 
                                control = list(weighting = function(x)
                                                weightTfIdf(x, 
                                                normalize =F),
                                          stopwords = TRUE))
party.tt <- tidy(party.tdm)
```

```{r, warning=FALSE, echo = FALSE}
library(shiny)

shinyApp(
    ui = fluidPage(
      fluidRow(style = "padding-bottom: 20px;",         
        column(4, sliderInput('year', 'Election Year', 3,
                               min = 1856, max = 2016, value=2016, step = 4)),
        column(4, sliderInput('nwords', 'Max Number', 3,
                               min = 6, max = 100, value=40, step = 5)),
        column(4, selectInput('party', 'Party', c("Democratic","Republican"),
                              selected="Democratic"))
      ),
      
      fluidRow(
        textOutput('text1'),
        plotOutput('wordclouds', height = "400px"),
        textOutput('text2')
      )
    ),

   server = function(input, output, session) {

      
    # Combine the selected variables into a new data frame
      selectedData <- reactive({
        
        party.sel <- (party.tt$document == paste0(input$year,"-",input$party,".txt"))
        
        pres.info <- findpresident(input$year)
        inaug.sel <- (inaug.tt$document == paste0("inaug",pres.info$File,"-",pres.info$Term,".txt"))
        
   
        list(party.term = party.tt$term[party.sel],  
             party.count = party.tt$count[party.sel],
             inaug.term = inaug.tt$term[inaug.sel],
             inaug.count =inaug.tt$count[inaug.sel],
             name = pres.info$President,
             pres.party = pres.info$Party
               )})
  
      output$wordclouds <- renderPlot(height = 400, {
        par(mfrow=c(1,2), mar = c(0, 0, 3, 0))
        
        wordcloud(selectedData()$party.term,
                  selectedData()$party.count,
              scale=c(4,0.5),
              max.words=input$nwords,
              min.freq=1,
              random.order=FALSE,
              rot.per=0,
              use.r.layout=FALSE,
              random.color=FALSE,
              colors=brewer.pal(10,"Blues")
              )
        wordcloud(selectedData()$inaug.term,
                  selectedData()$inaug.count,
              scale=c(4,0.5),
              max.words=input$nwords,
              min.freq=1,
              random.order=FALSE,
              rot.per=0,
              use.r.layout=FALSE,
              random.color=FALSE,
              colors=brewer.pal(10,"Blues")
              )
      })
      output$text1 <- renderText({paste0("FACT:  ",selectedData()$name,"(", selectedData()$pres.party,")"," won the Election in ", input$year, " ! ")})
      output$text2 <- renderText({paste0(input$party, " Party Platform VS the inauguation of new president ", selectedData()$name)})
    },

    options = list(height = 1500)
)
```



*b. Topic Modeling*

Usually, by word cloud, we can only do a qualitative research. Here, by using Topic Modeling, we can answer at least three questions: 
  
  1. In aspect of topics or focus, what is the significant point between the nominee and the party? 
  2. Who, among presidents in U.S.A., keeps the exactly same pace with his party? Who can be considered to ignore the party's announcement when he gave his inagual speech?
  3. How about other speeches given by the precidents, like farewells and nominees' speeches? 



For topic modeling, we prepare a corpus of sentence snipets as follows. For each speech, we start with sentences and prepare a snipet with a given sentence with the flanking sentences. 

```{r}

corpus.s.list=sentence.s.list[2:(nrow(sentence.s.list)-1), ]
sentence.s.pre=sentence.s.list$sentences[1:(nrow(sentence.s.list)-2)]
sentence.s.post=sentence.s.list$sentences[3:(nrow(sentence.s.list)-1)]
corpus.s.list$snipets=paste(sentence.s.pre, corpus.s.list$sentences, sentence.s.post, sep=" ") 
rm.rows=(1:nrow(corpus.s.list))[corpus.s.list$sent.id==1]  
rm.rows=c(rm.rows, rm.rows-1) 
corpus.s.list=corpus.s.list[-rm.rows, ]

  # Same process will be taken on 
corpus.p.list=sentence.p.list[2:(nrow(sentence.p.list)-1), ]
sentence.p.pre=sentence.p.list$sentences[1:(nrow(sentence.p.list)-2)]
sentence.p.post=sentence.p.list$sentences[3:(nrow(sentence.p.list)-1)]
corpus.p.list$snipets=paste(sentence.p.pre, corpus.p.list$sentences, sentence.p.post, sep=" ") 
rm.rows=(1:nrow(corpus.p.list))[corpus.p.list$sent.id==1]  
rm.rows=c(rm.rows, rm.rows-1)  
corpus.p.list=corpus.p.list[-rm.rows, ]
```

## Text mining
```{r}
allsnipets <- c(corpus.s.list$snipets, corpus.p.list$snipets)
docs <- Corpus(VectorSource(allsnipets))
#writeLines(as.character(docs[[sample(1:nrow(corpus.s.list), 1)]]))
```

### Text basic processing
Adapted from <https://eight2late.wordpress.com/2015/09/29/a-gentle-introduction-to-topic-modeling-using-r/>.

```{r}
#remove potentially problematic symbols
docs <-tm_map(docs,content_transformer(tolower))
#riteLines(as.character(docs[[sample(1:nrow(corpus.list), 1)]]))

#remove punctuation
docs <- tm_map(docs, removePunctuation)
#writeLines(as.character(docs[[sample(1:nrow(corpus.list), 1)]]))

#Strip digits
docs <- tm_map(docs, removeNumbers)
#writeLines(as.character(docs[[sample(1:nrow(corpus.list), 1)]]))

#remove stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
#writeLines(as.character(docs[[sample(1:nrow(corpus.list), 1)]]))

#remove whitespace
docs <- tm_map(docs, stripWhitespace)
#writeLines(as.character(docs[[sample(1:nrow(corpus.list), 1)]]))

#Stem document
docs <- tm_map(docs,stemDocument)
#writeLines(as.character(docs[[sample(1:nrow(corpus.list), 1)]]))
```

### Topic modeling

Gengerate document-term matrices. 

```{r}

tdm.beiyong <-  docs
tdm <- DocumentTermMatrix(docs)
#convert rownames to filenames#convert rownames to filenames
rownames(tdm) <- c(paste(corpus.s.list$type, corpus.s.list$File,
                       corpus.s.list$Term, corpus.s.list$sent.id, sep="_"),
                   paste(corpus.p.list$Year, corpus.p.list$Party, sep="_"))


rowTotals <- apply(tdm , 1, sum) 
# This line wil run for a lot of time due to the large scale of data
# But it's worth waiting since about 142 lines are removed after this step

nonempty1 <- rowTotals > 0 
tdm  <- tdm[nonempty1, ] 
n.corpuss <- nrow(corpus.s.list)
corpus.s.list <- corpus.s.list[nonempty1[1:n.corpuss], ]  
corpus.p.list <- corpus.p.list[nonempty1[(n.corpuss+1):length(nonempty1)], ]
nrow(tdm)
nrow(corpus.s.list) + nrow(corpus.p.list)
```

Run LDA

```{r}
#Set parameters for Gibbs sampling
burnin <- 800
iter <- 1500
thin <- 500
seed <-list(2003,5,63)
nstart <- 3
best <- TRUE

#Number of topics
k <- 12

#Run LDA using Gibbs sampling
ldaOut <-LDA(tdm, k, method="Gibbs", control=list(nstart=nstart, 
                                                 seed = seed, best=best,
                                                 burnin = burnin, iter = iter, 
                                                 thin=thin))
```



```{r}
#write out results
#docs to topics
ldaOut.topics <- as.matrix(topics(ldaOut))  # 每一句话都有了一个topic 
table(c(1:k, ldaOut.topics))
write.csv(ldaOut.topics,file=paste("../out/LDAGibbs",k,"DocsToTopics.csv"))

#top 10 terms in each topic
ldaOut.terms <- as.matrix(terms(ldaOut,10))
write.csv(ldaOut.terms,file=paste("../out/LDAGibbs",k,"TopicsToTerms.csv"))
ldaOut.terms
#probabilities associated with each topic assignment
topicProbabilities <- as.data.frame(ldaOut@gamma)
write.csv(topicProbabilities,file=paste("../out/LDAGibbs",k,"TopicProbabilities.csv"))
```



This algorithm works well, since for most of them, we can easily tell from the terms the possible categories of each column. After this, we can use heatmap to explore the relationship between different topics. 



```{r}
names(topicProbabilities) <- c("World","Legislation","Government","Econ",
                               "Health","commonword","Industry","Security","Nation", 
                               "Freedom","Politics","PublicAffair")
maxprob <- apply(topicProbabilities, 1, max)
max.matrix <- matrix(rep(maxprob, time = 10), ncol = 10)
## Using correlation to plot a heatmap to cluster K is not a good idea
heatmap.2(cor(topicProbabilities), 
          scale = "col", 
          col = bluered(100), margin=c(6, 6), key=F,
          trace = "none", density.info = "none")
```
We notice, some topic like Health, PublicAffair, Economics have deep relationships in the real word, so they will be brought up in the speeches together and hence the correlation is greater than 0; and PublicAffair is somewhat opposite to freedom due to some restrictions, we can notice their correlation is negative. Through similar analysis, this algorithm did give us a reasonable result that we can rely on. 


```{r}
## We can compare the value of posterior probability of a term belonging a certain topic to our confidence towards certain topic, which will be helpful for later description. 
par(mar=c(4, 6, 2, 1))
max.topic.probability <- apply(topicProbabilities, 1, max)
max.topic.names <- apply(topicProbabilities, 1, which.max)
confid <- data.frame(prob = max.topic.probability, 
                     nametopic = names(topicProbabilities)[max.topic.names])
confid <- confid[sample(1:nrow(confid), 1000, replace = F), ]
beeswarm(prob ~ nametopic, 
         data=confid,
         horizontal = TRUE,
         pch=16, col=alpha(brewer.pal(9, "Set1"), 0.6), 
         cex=1, cex.axis=1, cex.lab=0.8,
         spacing=5/nlevels(confid$nametopic),
         las=2, ylab="", xlab="Number of words in a sentence.",
         main="Inaugural Speeches")


```

This figure(beeswarm) shows us another point that prevent us from using some topics. When some terms became the one with greatest possibility, we hope this possibility is large so as to exclude the randomness of algorithm. Due to the big scale of our data, almost no point is below 0.1, which indicates a big chance of convergence of our algorithm. 


**TIPS**:Based on those two points, we will use this system of topics for the following analysis without dropping any topic for the moment. 

Here comes the first question:   1. In aspect of topics or focus, what is the significant point between the nominee and the party? 

Here is the algorithms 
i. We use the keyword, which is year, with metadata, to find the corresponding indices of both inauguration and party platform text. 
ii. Find corresponding row indices of posterior probability matrix produced by LDA, then compute the mean on each column, where we get the topic distribution of a certain article.
iii. Compute the difference between two distribution vectors, the result of which is the difference distribution of topics.
iii. Loop on each Election year, and finally we will get a matrix, which can be handled by heatmap.   


```{r, fig.width=2, fig.height=3}
  Years.compare <- seq(2016, 1856, -4)
  n.corpuss2 <- nrow(corpus.s.list) ## 19740
  n.totalrow <- nrow(topicProbabilities) ## 62750
  nrow(tdm)
  corpuss.s.topic <- cbind(corpus.s.list, topicProbabilities[1:n.corpuss2,])
  corpuss.p.topic <- cbind(corpus.p.list, topicProbabilities[(n.corpuss2+1):n.totalrow,])
  
  topic.pres <- matrix(NA, nrow = length(Years.compare), ncol = 12)
  colnames(topic.pres) <- names(topicProbabilities)
  r.name <- vector(length = length(Years.compare))
  
  for (i in 1:length(Years.compare)){
      ## Find the topics of the inaugspeech of presidents
      
      KEY1 <- findpresident(Years.compare[i])
      
      corpuss.temp <- tbl_df(corpuss.s.topic) %>%
             filter(File == (KEY1$File), type=="inaug") %>%
             select(25:36)
      corpuss.df1 <- as.data.frame(corpuss.temp)
      corpuss.df1m <- colMeans(corpuss.df1)
      
      ## Find that platform text
      
      corpuss.p.temp <-  tbl_df(corpuss.p.topic) %>%
             filter(Year == Years.compare[i], Party == KEY1$Party) %>%
             select(18:29)
      corpuss.df2 <- as.data.frame(corpuss.p.temp)
      corpuss.df2m <- colMeans(corpuss.df2)
      topic.pres[i,] <- corpuss.df2m - corpuss.df1m # Party - President
      
      r.name[i] <- paste0(KEY1$President,Years.compare[i] )
  }
  rownames(topic.pres) <- r.name
  
  
  # Draw a non-scaled figure
    heatmap.2(topic.pres, 
            scale = "none", 
            col = bluered(100), key=T, keysize = 2,
            cexRow = 0.5, cexCol = 0.5,
            trace = "none", density.info = "density")
  
  # Draw a column-scaled figure
    heatmap.2(topic.pres,
            scale = "colum", key=T,keysize = 2,
            col = bluered(100),
            cexRow = 0.5, cexCol = 0.5, 
            trace = "none", density.info = "density")
  
```

According to the code, a negative value means the president devote more proportion than his party to the corresponding topic, while red blocks are occupied by party platform texts.  


Lots of useful information can be obtained from those two figures. 

The figure without scaling directly telling me the topics presidents like and hate: presidents love talking about "world", "freedom" and enjoy using casual words, but they will be reluctant to discuss "health", "public affair" or "economics" in detail in their inauguration speeches. 

Another point is about trend. If we seriously look at the year of inauration, we will find in the past, generally speaking, president's speech didn't differ a lot from the announcements published by the party platfroms; however, recent presidents will focus more on more general things like global environments and instead the party will seriously discuss those important issues like economics and health care, most of which require lots of domain knowledge. 

Plus, both of figures indicate the distribution of topics appears like gaussian distribution, which fit the assumption of topic modeling. 


**Question2**: Who, among presidents in U.S.A., keeps the exactly same pace with his party? Who can be considered to ignore the party's announcement when he gave his inagual speech?

Answer: we can directly get the answer(at least a narrow range) from the figures above, or we can calculate the absolute value of each row.


```{r}
  ## Before Scaling
      score.before <- rowSums(abs(topic.pres))
      order.score.b <- order(score.before)
    #  rownames(topic.pres)[order.score.b]
    cat(names(which.min(score.before)), "said very similar things in this inauguration.\n")
    cat(names(which.max(score.before)),"almost ignored his party.")

```

Actually, this answer is not accurate due to the randomness of our algorithm and the measure errors. A much bette way is to do a *clustering* for some of the presidents. (Actually, the heatmap is already a method of clustering, but here using k-means method is easy to read.)

```{r}
    ## To see the name clearly, I remove the presidents' name, only leaving the year
    rr.name <- substr(r.name, nchar(r.name)-3,nchar(r.name))
    topicc.pres <- topic.pres
    rownames(topicc.pres) <- rr.name
    km.res=kmeans(topicc.pres, iter.max=200,5) 
fviz_cluster(km.res, 
             stand=F, repel= TRUE,
             data = topicc.pres, xlab="", xaxt=1,
             show.clust.cent=FALSE)
  
```

Presidents close to up-right corner(e.g. the red block) doesn't change topic a lot, while down-left corner(e.g. the blue block) gathers several presidents who would like to discuss different things from what the party platform mentioned before. 


 **Question3**: How about other speeches given by the presidents, like farewells and nominees' speeches? 

For this question, I will show you the case of nominees and the case of farewells one by one. 

i. Nominee speech(Remove those that didn't do this speech)
```{r,, fig.height=4, fig.width=3}

  topic.pres <- matrix(NA, nrow = length(Years.compare), ncol = 12)
  colnames(topic.pres) <- names(topicProbabilities)
  r.name <- vector(length = length(Years.compare))
 for (i in 1:length(Years.compare)){
      ## Find the topics of the inaugspeech of presidents

      KEY1 <- findpresident(Years.compare[i])
      # We choose those that will be presidents
      corpuss.temp <- tbl_df(corpuss.s.topic) %>%
             filter(File == (KEY1$File), type=="nomin") %>%
             select(25:36)
      corpuss.df1 <- as.data.frame(corpuss.temp)
      corpuss.df1m <- colMeans(corpuss.df1)

      ## Find that platform text

      corpuss.p.temp <-  tbl_df(corpuss.p.topic) %>%
             filter(Year == Years.compare[i], Party == KEY1$Party) %>%
             select(18:29)
      corpuss.df2 <- as.data.frame(corpuss.p.temp)
      corpuss.df2m <- colMeans(corpuss.df2)
      topic.pres[i,] <- corpuss.df2m - corpuss.df1m # Party - President

      r.name[i] <- paste0(KEY1$President,Years.compare[i] )
  }
  rownames(topic.pres) <- r.name
  KEY3 <- is.na(topic.pres[,"World"])
  topic.pres <- topic.pres[!KEY3,]
  length(topic.pres)
    heatmap.2(topic.pres, 
            scale = "none", 
            col = bluered(100), key=T, keysize = 2,
            cexRow = 1, cexCol = 0.6,
            trace = "none", density.info = "density")

    heatmap.2(topic.pres,
            scale = "colum", key=T,keysize = 2,
            col = bluered(100),
            cexRow = 1, cexCol = 0.6, 
            trace = "none", density.info = "density")
```
When those presidents were nominees, the only topic they speak a lot was commonword, which is not a real topic. This result can illustrate two things. First, the language used by announancement from party platform is more formal; Second, before being a president, a nominee actually would talk about almost everything in the speech, like economics and legislation, but once he became the president, he has to think about his everything. And it's president's duty to protect his citizens and their rights, no wonder "freedom" and "world" attract so much attention. 


ii. Farewell speech

```{r,, fig.height=3, fig.width=3}

  topic.pres <- matrix(NA, nrow = length(Years.compare), ncol = 12)
  colnames(topic.pres) <- names(topicProbabilities)
  r.name <- vector(length = length(Years.compare))
 for (i in 1:length(Years.compare)){
      ## Find the topics of the inaugspeech of presidents

      KEY1 <- findpresident(Years.compare[i]-4)
      # We choose those that will be presidents
      corpuss.temp <- tbl_df(corpuss.s.topic) %>%
             filter(File == (KEY1$File), type=="farewell") %>%
             select(25:36)
      corpuss.df1 <- as.data.frame(corpuss.temp)
      corpuss.df1m <- colMeans(corpuss.df1)

      ## Find that platform text

      corpuss.p.temp <-  tbl_df(corpuss.p.topic) %>%
             filter(Year == Years.compare[i], Party == KEY1$Party) %>%
             select(18:29)
      corpuss.df2 <- as.data.frame(corpuss.p.temp)
      corpuss.df2m <- colMeans(corpuss.df2)
      topic.pres[i,] <- corpuss.df2m - corpuss.df1m # Party - President

      r.name[i] <- paste0(KEY1$President,Years.compare[i] )
  }
  rownames(topic.pres) <- r.name
  KEY3 <- is.na(topic.pres[,"World"])
  topic.pres <- topic.pres[!KEY3,]
  length(topic.pres)
    heatmap.2(topic.pres, 
            scale = "none", 
            col = bluered(100), key=T, keysize = 2,
            cexRow = 1, cexCol = 0.6,
            trace = "none", density.info = "density")

    heatmap.2(topic.pres,
            scale = "colum", key=T,keysize = 2,
            col = bluered(100),
            cexRow = 1, cexCol = 0.6, 
            trace = "none", density.info = "density")
```
This figure looks like the plot from inauguration speech, indicating the position will mainly decide the topics of the speech. The difference between 




